import time
import pysubs2
import threading
import sys
import re
import os
from llama_cpp import Llama
from concurrent.futures import ThreadPoolExecutor

# --- é…ç½® ---
MODEL_PATH = r"E:\Downloads\sakura-7b-qwen2.5-v1.0-q6k.gguf"
INPUT_PATH = r"E:\Downloads\INPUT_ass.ass"  # æ”¯æŒ .srt æˆ– .ass
OUTPUT_PATH = r"E:\Downloads\output_fixed.ass"

MAX_WORKERS = 2
BATCH_SIZE = 6  # ASS ä¸“ç”¨çš„æ‰¹å¤„ç†å¤§å°
model_lock = threading.Lock()
progress_lock = threading.Lock()
completed_lines = 0
total_lines = 0

print(f"ğŸš€ å¯åŠ¨ SakuraLLMï¼šåŒæ¨¡è‡ªé€‚åº”å¼•æ“...")
llm = Llama(model_path=MODEL_PATH, n_gpu_layers=-1, n_ctx=1024, verbose=False)


# --- é€šç”¨å·¥å…·å‡½æ•° ---

def extract_raw_japanese(text):
    """
    æ™ºèƒ½æå–åŸæ–‡ï¼šä¿ç•™æ‰€æœ‰åŒ…å«æ—¥æ–‡å­—ç¬¦çš„è¡Œï¼Œå‰”é™¤ä¸å«æ—¥æ–‡çš„æ—§ç¿»è¯‘è¡Œã€‚
    """
    # ç»Ÿä¸€æ¢è¡Œç¬¦å¹¶åˆ†å‰²
    lines = text.replace('\\N', '\n').replace('[BR]', '\n').split('\n')
    # ç­›é€‰åŒ…å«å‡åï¼ˆå¹³å‡å/ç‰‡å‡åï¼‰çš„è¡Œ
    jp_lines = [l.strip() for l in lines if re.search(r'[\u3040-\u30ff]', l)]

    # å¦‚æœæ²¡æœåˆ°å‡åï¼ˆå¯èƒ½å…¨æ˜¯æ±‰å­—ï¼‰ï¼Œåˆ™é»˜è®¤å–ç¬¬ä¸€è¡Œ
    if not jp_lines and lines:
        return lines[0].strip()

    return "\\N".join(jp_lines)


def split_prefix_properly(text):
    """æå–è¡Œé¦–åå­—æ‹¬å·/ç‰¹æ®Šç¬¦å·å‰ç¼€"""
    pattern = r'^((?:\{.*?\}|[-ï¼\s]*[ï¼ˆ\(].*?[ï¼‰\)]+[\s]*|[-ï¼]+|[^\w\u4e00-\u9fa5\u3041-\u30ff]+)+)'
    match = re.match(pattern, text)
    if match:
        prefix = match.group(1)
        body = text[len(prefix):].strip()
        return prefix, body
    return "", text


def clean_mid_text_furigana(text):
    """æ¸…ç†æ–‡ä¸­æ³¨éŸ³"""
    return re.sub(r'([\u4e00-\u9fa5])[\(ï¼ˆ][\u3040-\u30ff]+[\)ï¼‰]', r'\1', text)


def update_progress(increment=1):
    global completed_lines
    with progress_lock:
        completed_lines += increment
        percent = (completed_lines / total_lines) * 100
        sys.stdout.write(f"\rğŸ“ˆ è¿›åº¦: {percent:.1f}% ({completed_lines}/{total_lines})")
        sys.stdout.flush()


# --- æ–¹æ¡ˆ A: SRT é€»è¾‘ (å¹¶å‘å•è¡Œ) ---

def translate_line_srt(idx, all_tasks):
    line_obj, original_text = all_tasks[idx]

    # ã€ä¿®æ”¹ç‚¹ã€‘ä½¿ç”¨æ™ºèƒ½æå–å‡½æ•°ï¼Œæ”¯æŒå¤šè¡Œæ—¥æ–‡ä¸”é˜²æ­¢é‡å¤
    original_jp = extract_raw_japanese(original_text)

    prefix, pure_body = split_prefix_properly(original_jp)
    clean_body = clean_mid_text_furigana(pure_body)

    if not clean_body.strip():
        line_obj.text = f"{original_jp}\\N{prefix}"
        return update_progress()

    prompt = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªæ—¥æ¼«ç¿»è¯‘ï¼Œè¯·å°†æ—¥æ–‡ç¿»è¯‘æˆæµåˆ©çš„ä¸­æ–‡å°è¯ã€‚ç›´æ¥è¾“å‡ºè¯‘æ–‡ã€‚<|im_end|>\n<|im_start|>user\n{clean_body}<|im_end|>\n<|im_start|>assistant\n"

    try:
        with model_lock:
            output = llm(prompt, max_tokens=150, stop=["<|im_end|>", "\n"], temperature=0.1, repeat_penalty=1.2)
        res = output["choices"][0]["text"].strip()

        # ç»“æœæ¸…æ´—ï¼šå»æ‰ AI å¼•å¯¼è¯­ã€å ä½ç¬¦ç¢ç‰‡å’Œå­¤ç«‹ç¬¦å·
        res = re.sub(r'ç¿»è¯‘ç»“æœ|è¯‘æ–‡|ç¿»è¯‘[:ï¼š]|ã€Œ|ã€', '', res).strip()
        res = re.sub(r'\[?T\d+\]?|\[T|T\]', '', res)  # æ¸…ç†å ä½ç¬¦
        res = res.replace('[', '').replace(']', '')  # å…¨å±€æ¸…ç†ä¸­æ‹¬å·
        res = re.sub(r'(^|\\N)[-ï¼\s]+', r'\1', res)  # æ¸…ç†è¡Œé¦–æ¨ªæ 
        res = res.strip()

        if res:
            final_zh = f"{prefix}{res}".replace("[BR]", "\\N")
            line_obj.text = f"{original_jp}\\N{final_zh}"
        else:
            line_obj.text = f"{original_jp}\\N{prefix}{clean_body}"
        update_progress()
    except Exception:
        line_obj.text = original_jp
        update_progress()


# --- æ–¹æ¡ˆ B: ASS é€»è¾‘ (æ‰¹å¤„ç†ä¿æŠ¤) ---

def protect_ass_tags(text):
    """æå–æ ‡ç­¾å ä½ç¬¦ï¼Œå¹¶å¢å¼ºè¡Œå°¾ç¬¦å·ä¿æŠ¤"""
    prefix, rem_text = split_prefix_properly(text)
    suffix_pattern = r'([â¡â‰«ã€‹>]+)$'
    suffix_match = re.search(suffix_pattern, rem_text)
    suffix = suffix_match.group(1) if suffix_match else ""
    body = rem_text[:-len(suffix)] if suffix else rem_text

    tags = re.findall(r'\{.*?\}', body)
    masked_body = body
    for i, tag in enumerate(tags):
        masked_body = masked_body.replace(tag, f" [T{i}] ", 1)

    masked_body = clean_mid_text_furigana(masked_body)
    return prefix, masked_body, tags, suffix


def translate_batch_ass(batch_tasks):
    prompt_lines = []
    for idx, task in enumerate(batch_tasks):
        if not re.search(r'[\u3040-\u30ff\u4e00-\u9fa5]', task['masked_body']):
            prompt_lines.append(f"{idx + 1}: SKIP_LINE")
        else:
            prompt_lines.append(f"{idx + 1}: {task['masked_body']}")

    combined_input = "\n".join(prompt_lines)

    prompt = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªæ—¥æ¼«ç¿»è¯‘ä¸“å®¶ã€‚è¯·æŒ‰ç¼–å·ç¿»è¯‘å°è¯ï¼Œä¿æŒ[T_n]å ä½ç¬¦ä½ç½®ä¸å˜ã€‚å¦‚æœè¾“å…¥åŒ…å«\\Næ¢è¡Œï¼Œè¯·å¯¹åº”ä¿ç•™ã€‚å¦‚æœçœ‹åˆ° SKIP_LINE åˆ™åŸæ ·è¾“å‡ºã€‚<|im_end|>\n<|im_start|>user\n{combined_input}<|im_end|>\n<|im_start|>assistant\n1: "

    with model_lock:
        output = llm(prompt, max_tokens=1024, temperature=0.1, stop=["<|im_end|>", "User:"])

    raw_res = "1: " + output["choices"][0]["text"]
    results = {}
    for i in range(len(batch_tasks)):
        pattern = rf"{i + 1}[:ï¼š]\s*(.*?)(?=\n\d+[:ï¼š]|$)"
        match = re.search(pattern, raw_res, re.DOTALL)

        if match:
            content = match.group(1).strip()

            # 1. åŸºç¡€æ‹†è§£ï¼šåˆ æ‰ ID å’Œ å ä½ç¬¦ç¢ç‰‡
            content = re.sub(r'^\d+[:ï¼š.]\s*', '', content)
            content = re.sub(r'\[?T\d+\]?|\[T|T\]', '', content)
            content = content.replace('[', '').replace(']', '')
            content = re.sub(r'ç¿»è¯‘ç»“æœ|è¯‘æ–‡|ã€Œ|ã€|SKIP_LINE', '', content).strip()

            # 2. é•¿åº¦ç†”æ–­ï¼šå¦‚æœç¿»è¯‘æ¯”åŸæ–‡é•¿ 5 å€ä¸”è¶…è¿‡ 50 å­—ï¼Œåˆ¤å®šä¸ºç™¾ç§‘å¹»è§‰
            # è¿™ä¸€æ­¥æ”¾åœ¨å»é‡ä¹‹å‰ï¼Œå› ä¸ºå»é‡åé•¿åº¦ä¼šå˜çŸ­ï¼Œå¯èƒ½æ¼æ‰å¹»è§‰åˆ¤æ–­
            if len(content) > 50 and len(content) > len(task['masked_body']) * 5:
                # ä»…ä¿ç•™ç¬¬ä¸€å¥ï¼Œé˜²æ­¢ç™¾ç§‘è¯æ¡å å±
                parts = re.split(r'[ï¼Œã€‚ï¼]', content)
                content = parts[0] + "..."

            # 3. è¿ç»­å¤è¯»æ¸…æ´— (é’ˆå¯¹ï¼šå¥½ç—›å¥½ç—›å¥½ç—›...)
            # å¤„ç†çŸ­è¯­å¤è¯» (3-15ä¸ªå­—é‡å¤3æ¬¡ä»¥ä¸Š)
            content = re.sub(r'(.{3,15}?)(\1){2,}', r'\1\1...', content)
            # å¤„ç†å•å­—/çŸ­è¯å¤è¯» (1-3ä¸ªå­—é‡å¤4æ¬¡ä»¥ä¸Š)
            content = re.sub(r'(.+?)\1{4,}', r'\1\1\1...', content)

            # 4. å°¾éƒ¨åƒåœ¾ä¸æ•°å­—æ¸…ç†
            # åˆ æ‰è¡Œå°¾å­¤ç«‹æ•°å­— (è§£å†³ 2 çš„é—®é¢˜)
            content = re.sub(r'[\s\\N]*\d{1,2}$', '', content)
            # åˆ æ‰è¡Œé¦–/æ¢è¡Œåçš„æ¨ªæ 
            content = re.sub(r'(^|\\N)[-ï¼\s]+', r'\1', content)

            # 5. ç¬¦å·è¡Œç‰¹æ®Šä¿æŠ¤
            # å¦‚æœåŸæ–‡æ²¡æ±‰å­—/å‡åï¼Œè¯‘æ–‡ä¹Ÿä¸è¯¥æœ‰æ•°å­—
            if not re.search(r'[\u3040-\u30ff\u4e00-\u9fa5]', task['masked_body']):
                content = re.sub(r'\d+', '', content)

            results[i] = content.strip()
    return results


# --- å…¥å£æ§åˆ¶ ---

def start():
    global total_lines
    ext = os.path.splitext(INPUT_PATH)[1].lower()
    subs = pysubs2.load(INPUT_PATH)

    all_tasks = []
    for line in subs:
        p = line.plaintext.strip()
        if p:
            # æ­¤å¤„ä¸ç›´æ¥åˆ†è¡Œï¼Œç”±ç¿»è¯‘é€»è¾‘å†…éƒ¨å¤„ç†å¤šè¡Œå…³ç³»
            p = p.replace("\\N", "[BR]").replace("\n", "[BR]")
            all_tasks.append((line, p))

    total_lines = len(all_tasks)
    time_s = time.time()

    if ext == ".ass":
        print(f"ğŸ¬ æ£€æµ‹åˆ° ASS æ ¼å¼ï¼Œå¯åŠ¨ã€æ‰¹å¤„ç†ä¿æŠ¤ã€‘æ–¹æ¡ˆ...")
        ass_tasks = []
        for line_obj, text in all_tasks:
            # ã€ä¿®æ”¹ç‚¹ã€‘å›å¡«å‰å…ˆæ´—ä¸€éåŸæ–‡ï¼Œæ”¯æŒå¤šè¡Œæ—¥æ–‡
            raw_jp = extract_raw_japanese(text)
            prefix, masked_body, tags, suffix = protect_ass_tags(raw_jp)
            ass_tasks.append({
                'obj': line_obj,
                'masked_body': masked_body,
                'prefix': prefix,
                'tags': tags,
                'suffix': suffix,
                'raw_jp': raw_jp
            })

        for i in range(0, total_lines, BATCH_SIZE):
            batch = ass_tasks[i: i + BATCH_SIZE]
            batch_results = translate_batch_ass(batch)
            for idx, task in enumerate(batch):
                res = batch_results.get(idx, "")
                if res:
                    for t_idx, tag in enumerate(task['tags']):
                        res = res.replace(f"[T{t_idx}]", tag).replace(f"T{t_idx}", tag)

                    res = re.sub(r'([ã€Šï¼ˆ(])\1+', r'\1', res)
                    final_zh = f"{task['prefix']}{res}{task['suffix']}"
                    final_zh = final_zh.replace(" ", "").replace("[BR]", "\\N")

                    # ã€ä¿®æ”¹ç‚¹ã€‘ç»Ÿä¸€ä½¿ç”¨æ¸…æ´—åçš„ raw_jp è¿›è¡Œå›å¡«
                    task['obj'].text = f"{task['raw_jp']}\\N{final_zh}"
                else:
                    task['obj'].text = task['raw_jp']
            update_progress(len(batch))

    else:
        print(f"ğŸ“ æ£€æµ‹åˆ° SRT æ ¼å¼ï¼Œå¯åŠ¨ã€å¹¶å‘å•è¡Œã€‘æ–¹æ¡ˆ...")
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            list(executor.map(lambda i: translate_line_srt(i, all_tasks), range(total_lines)))

    subs.save(OUTPUT_PATH)
    time_elapsed = time.time() - time_s
    print(f"\nâœ¨ å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_PATH} ç”¨æ—¶ï¼š{time_elapsed:.2f}s")


if __name__ == "__main__":
    start()
